{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (1.11.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (1.5.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (3.9.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in d:\\compilers\\python\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aman tripathi\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\compilers\\python\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aman tripathi\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\compilers\\python\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\compilers\\python\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\compilers\\python\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aman tripathi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman Tripathi\\AppData\\Local\\Temp\\ipykernel_4284\\4116815093.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(str)  # Convert all values to string format\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Load the \"Adult\" dataset\n",
    "data = fetch_openml('adult', version=2)\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Preprocess data: Convert categorical features into transactional format\n",
    "df = df[['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']]\n",
    "df = df.applymap(str)  # Convert all values to string format\n",
    "\n",
    "# Transaction Encoder to create 0/1 encoding for transactional representation\n",
    "te = TransactionEncoder()\n",
    "df_encoded = te.fit(df.values).transform(df.values)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Load the \"Adult\" dataset\n",
    "data = fetch_openml('adult', version=2)\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "# Convert all values to string format\n",
    "\n",
    "# Preprocess data: Convert categorical features into transactional format\n",
    "df = df[['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']]\n",
    "\n",
    "df = df.astype(str)\n",
    "\n",
    "# Transaction Encoder to create 0/1 encoding for transactional representation\n",
    "te = TransactionEncoder()\n",
    "df_encoded = te.fit(df.values).transform(df.values)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support                itemsets\n",
      "11  0.897424         (United-States)\n",
      "12  0.855043                 (White)\n",
      "44  0.788113  (United-States, White)\n",
      "9   0.694198               (Private)\n",
      "4   0.668482                  (Male)\n"
     ]
    }
   ],
   "source": [
    "# Apply Apriori with minimum support of 0.15\n",
    "frequent_itemsets_apriori = apriori(df_encoded, min_support=0.15, use_colnames=True)\n",
    "\n",
    "# Sort by support and list the top 5 frequent itemsets\n",
    "top_5_frequent_itemsets = frequent_itemsets_apriori.nlargest(5, 'support')\n",
    "print(top_5_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fim-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading fim_python-1.2.2.tar.gz (341 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in d:\\compilers\\python\\lib\\site-packages (from fim-python) (1.26.0)\n",
      "Collecting Cython>=0.29.22 (from fim-python)\n",
      "  Using cached Cython-3.0.11-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Using cached Cython-3.0.11-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "Building wheels for collected packages: fim-python\n",
      "  Building wheel for fim-python (pyproject.toml): started\n",
      "  Building wheel for fim-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build fim-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for fim-python (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      ['/O2']\n",
      "      []\n",
      "      ['/O2']\n",
      "      []\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fim-python\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (fim-python)\n"
     ]
    }
   ],
   "source": [
    "pip install fim-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement fim (from versions: none)\n",
      "ERROR: No matching distribution found for fim\n"
     ]
    }
   ],
   "source": [
    "pip install fim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m genmax\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# GenMax Algorithm (pseudocode; you may need to install and adapt for Python)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m genmax_results \u001b[38;5;241m=\u001b[39m genmax(df_encoded\u001b[38;5;241m.\u001b[39mvalues, supp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, zmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fim'"
     ]
    }
   ],
   "source": [
    "from fim import genmax\n",
    "\n",
    "# GenMax Algorithm (pseudocode; you may need to install and adapt for Python)\n",
    "genmax_results = genmax(df_encoded.values, supp=15, zmin=1)\n",
    "\n",
    "# Display the top 5 maximal frequent itemsets (assuming 'genmax_results' returns itemsets)\n",
    "top_5_genmax = genmax_results[:5]\n",
    "print(top_5_genmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal frequent itemsets: [['workclass'], ['relationship'], ['age'], ['sex'], ('sex', 'age')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Function to calculate the support of itemsets\n",
    "def calculate_support(itemset, transactions):\n",
    "    count = 0\n",
    "    for transaction in transactions:\n",
    "        if set(itemset).issubset(set(transaction)):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Function to generate candidates\n",
    "def generate_candidates(frequent_itemsets, length):\n",
    "    return list(combinations(set().union(*frequent_itemsets), length))\n",
    "\n",
    "# Recursive function to find maximal frequent itemsets\n",
    "def genmax(transactions, min_support):\n",
    "    frequent_itemsets = []  # List to store the maximal frequent itemsets\n",
    "    candidates = [[item] for item in set().union(*transactions)]\n",
    "    \n",
    "    # Loop until no more candidates\n",
    "    length = 1\n",
    "    while candidates:\n",
    "        next_frequent_itemsets = []\n",
    "        \n",
    "        # Calculate support for each candidate\n",
    "        for candidate in candidates:\n",
    "            support = calculate_support(candidate, transactions) / len(transactions)\n",
    "            if support >= min_support:\n",
    "                next_frequent_itemsets.append(candidate)\n",
    "        \n",
    "        # Check for maximal itemsets\n",
    "        for itemset in next_frequent_itemsets:\n",
    "            if not any(set(itemset).issubset(set(other)) for other in frequent_itemsets):\n",
    "                frequent_itemsets.append(itemset)\n",
    "        \n",
    "        # Generate candidates for the next level\n",
    "        length += 1\n",
    "        candidates = generate_candidates(next_frequent_itemsets, length)\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Example dataset (transactional format)\n",
    "transactions = [\n",
    "    ['age', 'workclass', 'education'],\n",
    "    ['age', 'sex'],\n",
    "    ['workclass', 'occupation', 'relationship'],\n",
    "    ['age', 'relationship', 'sex'],\n",
    "]\n",
    "\n",
    "# Set the minimum support threshold (e.g., 50%)\n",
    "min_support = 0.5\n",
    "\n",
    "# Apply the GenMax algorithm\n",
    "maximal_frequent_itemsets = genmax(transactions, min_support)\n",
    "\n",
    "# Output the maximal frequent itemsets\n",
    "print(\"Maximal frequent itemsets:\", maximal_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycharm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycharm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m charm_algorithm  \u001b[38;5;66;03m# Hypothetical import\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Charm Algorithm to find closed frequent itemsets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m closed_itemsets \u001b[38;5;241m=\u001b[39m charm_algorithm(df_encoded\u001b[38;5;241m.\u001b[39mvalues, supp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycharm'"
     ]
    }
   ],
   "source": [
    "from pycharm import charm_algorithm  # Hypothetical import\n",
    "\n",
    "# Charm Algorithm to find closed frequent itemsets\n",
    "closed_itemsets = charm_algorithm(df_encoded.values, supp=0.15)\n",
    "\n",
    "# Display the top 5 closed frequent itemsets\n",
    "top_5_closed_itemsets = closed_itemsets[:5]\n",
    "print(top_5_closed_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "class myclass:\n",
    "    def __init__(self,x):\n",
    "        self.x = x\n",
    "    def my_method(self):\n",
    "        print(self.x)\n",
    "\n",
    "\n",
    "obj = myclass(10)\n",
    "obj.my_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocking Probability for M/M/200/200  system with offred traffic 180 is: 0.010325 or 1.0324995204982297% \n",
      "The smallest number of circuits that should be added to meet the 1% requirement is 1\n"
     ]
    }
   ],
   "source": [
    "#question 2:-\n",
    "from math import factorial\n",
    "\n",
    "\n",
    "def erlang_b(A, C):\n",
    "    numerator = (A**C) / factorial(C)\n",
    "    denominator = sum((A**k) / factorial(k) for k in range(C + 1))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "A = 180  # Load offered as given in question\n",
    "C = 200  # Number of servers which is same as number of circuits given in question\n",
    "\n",
    "prob_block = erlang_b(A, C)\n",
    "print(f\"Blocking Probability for M/M/{C}/{C}  system with offred traffic {A} is: {prob_block:.6f} or {prob_block * 100}% \")\n",
    "\n",
    "#code for finding the no of channels to be added to make the blocking probability less than 1%\n",
    "\n",
    "\n",
    "target_blocking_probability = 0.01  \n",
    "\n",
    "while erlang_b(A, C) > target_blocking_probability:\n",
    "    C += 1\n",
    "print(f\"The smallest number of circuits that should be added to meet the 1% requirement is {C-200}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'E_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((E \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m k) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mfactorial(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N)) \u001b[38;5;241m+\u001b[39m numerator\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numerator \u001b[38;5;241m/\u001b[39m denominator\n\u001b[1;32m---> 15\u001b[0m analytical_b \u001b[38;5;241m=\u001b[39m [erlang_b(E, N) \u001b[38;5;28;01mfor\u001b[39;00m E \u001b[38;5;129;01min\u001b[39;00m \u001b[43mE_values\u001b[49m]\n\u001b[0;32m     16\u001b[0m analytical_c \u001b[38;5;241m=\u001b[39m [erlang_c(E, N) \u001b[38;5;28;01mfor\u001b[39;00m E \u001b[38;5;129;01min\u001b[39;00m E_values]\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'E_values' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def erlang_b(E, N):\n",
    "    numerator = (E ** N) / math.factorial(N)\n",
    "    denominator = sum((E ** k) / math.factorial(k) for k in range(N + 1))\n",
    "    return numerator / denominator\n",
    "\n",
    "def erlang_c(E, N):\n",
    "    if E >= N:\n",
    "        return 1.0\n",
    "    numerator = (E ** N) / math.factorial(N) * (N / (N - E))\n",
    "    denominator = sum((E ** k) / math.factorial(k) for k in range(N)) + numerator\n",
    "    return numerator / denominator\n",
    "\n",
    "analytical_b = [erlang_b(E, N) for E in E_values]\n",
    "analytical_c = [erlang_c(E, N) for E in E_values]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Erlang B\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(E_values, analytical_b, label='Analytical Erlang B', linestyle='--', color='blue')\n",
    "plt.plot(E_values, simulated_b, label='Simulated Erlang B', marker='o', color='green', markersize=4)\n",
    "plt.title('Erlang B Blocking Probability')\n",
    "plt.xlabel('Traffic Intensity (E)')\n",
    "plt.ylabel('Blocking Probability')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot Erlang C\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(E_values, analytical_c, label='Analytical Erlang C', linestyle='--', color='blue')\n",
    "plt.plot(E_values, simulated_c, label='Simulated Erlang C', marker='o', color='green', markersize=4)\n",
    "plt.title('Erlang C Waiting Probability')\n",
    "plt.xlabel('Traffic Intensity (E)')\n",
    "plt.ylabel('Waiting Probability')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
