{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Preprocess the training dataset\n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "train_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\n",
    "train_data['Embarked'] = train_data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "train_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# Preprocess the testing dataset\n",
    "test_data['Age'].fillna(test_data['Age'].median(), inplace=True)\n",
    "test_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Sex'] = test_data['Sex'].map({'male': 0, 'female': 1})\n",
    "test_data['Embarked'] = test_data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "test_data.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# Define features and target for training data\n",
    "X = train_data.drop('Survived', axis=1).values\n",
    "y = train_data['Survived'].values\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(X_train, y_train, learning_rate=0.02, num_iterations=800):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros(n)\n",
    "    cost_history = []\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(X, y, theta):\n",
    "        m = len(y)\n",
    "        predictions = sigmoid(np.dot(X, theta))\n",
    "        cost = -1/m * (np.dot(y, np.log(predictions + 1e-10)) + np.dot((1 - y), np.log(1 - predictions + 1e-10)))\n",
    "        return cost\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        linear_model = np.dot(X_train, theta)\n",
    "        predictions = sigmoid(linear_model)\n",
    "        gradients = np.dot(X_train.T, (predictions - y_train)) / m\n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "        # Compute and store the cost for plotting\n",
    "        cost = compute_cost(X_train, y_train, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Print cost after every 100 iterations\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1}/{num_iterations}, Cost: {cost}\")\n",
    "\n",
    "    return theta, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_predict(X_test, theta):\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    linear_model = np.dot(X_test, theta)\n",
    "    predictions = sigmoid(linear_model)\n",
    "    return [1 if i > 0.5 else 0 for i in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_function(cost_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(cost_history)), cost_history, color='blue')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost Function Over Iterations')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_bias_term(theta):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(theta, 'o-', color='red')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Parameter Value (Theta)')\n",
    "    plt.title('Bias Term (Theta) Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(test_data.values)\n",
    "\n",
    "# Train the model\n",
    "theta, cost_history = logistic_regression_train(X_train, y_train, learning_rate=0.02, num_iterations=800)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = logistic_regression_predict(X_val, theta)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Plot the cost function\n",
    "plot_cost_function(cost_history)\n",
    "confusion_matrix = metrics.confusion_matrix(y_val, y_val_pred)\n",
    "# Plot the bias term (theta) values\n",
    "print(confusion_matrix)\n",
    "# Display predictions\n",
    "print('Predictions:', logistic_regression_predict(X_test, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_predict(X_test, theta):\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    linear_model = np.dot(X_test, theta)\n",
    "    predictions = sigmoid(linear_model)\n",
    "    return [1 if i > 0.5 else 0 for i in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bias term (intercept)\n",
    "bias_term = theta[0]\n",
    "\n",
    "# Print the bias term\n",
    "print(f\"Bias Term (Intercept): {bias_term}\")\n",
    "\n",
    "# Visualize the theta values including the bias term\n",
    "plot_bias_term(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(X_train, y_train, learning_rate=0.02, num_iterations=800):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros(n)  # Initialize theta as a vector of zeros\n",
    "    cost_history = []\n",
    "    theta_history = []\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(X, y, theta):\n",
    "        m = len(y)\n",
    "        predictions = sigmoid(np.dot(X, theta))\n",
    "        cost = -1/m * (np.dot(y, np.log(predictions + 1e-10)) + np.dot((1 - y), np.log(1 - predictions + 1e-10)))\n",
    "        return cost\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        linear_model = np.dot(X_train, theta)\n",
    "        predictions = sigmoid(linear_model)\n",
    "        gradients = np.dot(X_train.T, (predictions - y_train)) / m\n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "        # Compute and store the cost for plotting\n",
    "        cost = compute_cost(X_train, y_train, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Store the current theta values (including bias term)\n",
    "        theta_history.append(theta.copy())\n",
    "        \n",
    "        # Print cost and theta after every 100 iterations\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1}/{num_iterations}, Cost: {cost}, Theta: {theta}\")\n",
    "\n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "# Train the model and track theta values\n",
    "theta, cost_history, theta_history = logistic_regression_train(X_train, y_train, learning_rate=0.02, num_iterations=800)\n",
    "\n",
    "# Extract bias term from theta history\n",
    "bias_history = [theta[0] for theta in theta_history]\n",
    "\n",
    "# Plot the bias term over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(bias_history)), bias_history, color='green')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Bias Term (Theta[0])')\n",
    "plt.title('Bias Term (Theta[0]) Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the parameter vector (theta) changes over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(1, len(theta)):\n",
    "    plt.plot(range(len(theta_history)), [theta[i] for theta in theta_history], label=f'Theta[{i}]')\n",
    "\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Theta Values')\n",
    "plt.title('Parameter Vector (Theta) Changes Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
